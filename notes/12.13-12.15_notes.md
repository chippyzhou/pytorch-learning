### Tensors（张量）
#### Conception
- a data structure similar to arrays and matrices -- can see as **parameters**
- use it to encode inputs and outputs
- run on GPUs to accelerate computing
#### Initialization
```python
# Directly from data
data = [[1, 2], [3, 4]]
x_data = torch.tensor(data)

# From a Numpy array
np_array = np.array(data)
x_np = torch.from_numpy(np_array)

# From another tensor
x_ones = torch.ones_like(x_data) # retains the properties of x_data
x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data

# With random or constant values
shape = (2, 3,)
rand_tensor = torch.rand(shape)
ones_tensor = torch.ones(shape)
zeros_tensor = torch.zeros(shape)
```
#### Attributes
```python
# describes:
tensor = torch.rand(3, 4)

print(f"Shape of tensor: {tensor.shape}") # torch.Size([3,4])
print(f"Datatype of tensor: {tensor.dtype}") # torch.float32
print(f"Device tensor is stored on: {tensor.device}") # cpu
```
#### Operations
- indexing, linear algebra... https://docs.pytorch.org/docs/stable/torch.html
- run on GPU in mac--**colab**  https://colab.research.google.com
```python
# indexing and slicing
tensor = torch.ones(4, 4)
tensor[:,1] = 0 # [row, column]/: means all/1 means the second column
print(tensor)

# joining tensors(Concatenate)
t1 = torch.cat([tensor, tensor, tensor], dim=1) # 0竖着拼 1横着拼
print(t1)

# multiplying tensors（张量乘法）
y = tensor * tensor # 对应位置相乘
y = tensor @ tensor # 矩阵乘法

# in-place operations（原地修改）判别：方法加了下划线 
x.add(5) vs x.add_(5)
```
### Autograd
#### Conception
- engine to power neural network training (auto gradient)
#### Background
- nested functions
- forward propagation -- inputs->functions->outputs
- backword propagation -- error->gradients->adjust
#### Usage in Pytorch
- a single training as example:
```python
import torch
from torchvision.models import resnet18, ResNet18_Weights # pretrained data
model = resnet18(weights=ResNet18_Weights.DEFAULT)
data = torch.rand(1, 3, 64, 64)
labels = torch.rand(1, 1000)

prediction = model(data) # forward pass

loss = (prediction - labels).sum() # calculate the loss
loss.backward() # backward pass

optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9) # optimizer
# add momentum to optimize more fast：90% the last direction, 10% the new gradient
# lr：leaning rate -- the size of pace

optim.step() #gradient descent -- adjust parameter by gradient
```
#### Differentiation in Autograd
```python
import torch

a = torch.tensor([2., 3.], requires_grad=True) # 参数开追踪用于后面的backward优化参数
b = torch.tensor([6., 4.], requires_grad=True)

Q = 3*a**3 - b**2 # prediction

# gradient is a tensor of the same shape as Q
external_grad = torch.tensor([1., 1.]) 

Q.backward(gradient=external_grad) # backwards只能用来做标量微分 external_grad将分量加权求和再微分，以处理向量

# check if collected gradients are correct
print(9*a**2 == a.grad)
print(-2*b == b.grad)
```
#### Optional Reading - Vector Calculus using `autograd`
$$\text{最终的梯度} = \text{雅可比矩阵 } J^T \times \text{你传入的向量 } \vec{v}$$
$$(高数忘光了不展开了）$$
#### Computational Graph--Directed Acyclic Graph
- 略，还是底层原理
#### Exclusion from the DAG
```python
x = torch.rand(5, 5)
y = torch.rand(5, 5)
z = torch.rand((5, 5), requires_grad=True)

a = x + y
print(f"Does `a` require gradients?: {a.requires_grad}")
b = x + z
print(f"Does `b` require gradients?: {b.requires_grad}")

from torch import nn, optim

model = resnet18(weights=ResNet18_Weights.DEFAULT)

# Freeze all the parameters in the network
for param in model.parameters():
    param.requires_grad = False # 冻结了 因为已知的参数已经很好了
    
model.fc = nn.Linear(512, 10) # 全连接层 识别512个特征 输出10种分类（输出是require gradients的）

# Optimize only the classifier
optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)
```
### Neural Networks
#### Constructed
- using the `torch.nn` package
- a simple neural network -- inputs feeds many layers, finally gives the outputs ![[mnist.png]]
#### Procedure
1. Define the neural network that has some learnable **parameters** (or weights)
2. **Iterate over** a dataset of inputs
3. Process **input** through the network
4. **Compute the loss** (how far is the output from being correct)
5. **Propagate gradients** back into the network’s parameters
6. **Update the weights** of the network, typically using a simple update rule: 
$$weight = weight - learning_{rate} * gradient$$
#### Define the network
```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):

    def __init__(self):  # constructor
        super(Net, self).__init__() # 调用初始化函数
        # 1 input image channel, 6 output channels, 5x5 square convolution
        # kernel
        self.conv1 = nn.Conv2d(1, 6, 5) # 两个卷积层--扫描图片提取特征
        self.conv2 = nn.Conv2d(6, 16, 5) # 输入通道数 提取特征数 卷积核大小n*n
        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 5 * 5, 120)  
        # 5*5 from image dimension 16是conv2的输出*卷积池化后的图像尺寸
        # 全连接层只认向量，所以要拉长成向量，所以用乘
        self.fc2 = nn.Linear(120, 84) # 全连接层（Fully connected）
        self.fc3 = nn.Linear(84, 10) # 输入和输出数

    def forward(self, input):
        # Convolution layer C1: 1 input image channel, 6 output channels,
        # 5x5 square convolution, it uses RELU activation function, and
        # outputs a Tensor with size (N, 6, 28, 28), where N is the size of the batch
        c1 = F.relu(self.conv1(input)) # 卷积--激活函数 把负数变成0
        # Subsampling layer S2: 2x2 grid, purely functional,
        # this layer does not have any parameter, and outputs a (N, 6, 14, 14) Tensor
        s2 = F.max_pool2d(c1, (2, 2)) # 池化--把图片缩小到原来的一半（/2）
        # Convolution layer C3: 6 input channels, 16 output channels,
        # 5x5 square convolution, it uses RELU activation function, and
        # outputs a (N, 16, 10, 10) Tensor
        c3 = F.relu(self.conv2(s2)) # 同上
        # Subsampling layer S4: 2x2 grid, purely functional,
        # this layer does not have any parameter, and outputs a (N, 16, 5, 5) Tensor
        s4 = F.max_pool2d(c3, 2)
        # Flatten operation: purely functional, outputs a (N, 400) Tensor
        s4 = torch.flatten(s4, 1) # 为了进全连接层--降维
        # Fully connected layer F5: (N, 400) Tensor input,
        # and outputs a (N, 120) Tensor, it uses RELU activation function
        f5 = F.relu(self.fc1(s4)) 
        # Fully connected layer F6: (N, 120) Tensor input, # 这里也提到了 张量输入
        # and outputs a (N, 84) Tensor, it uses RELU activation function
        f6 = F.relu(self.fc2(f5))
        # Fully connected layer OUTPUT: (N, 84) Tensor input, and
        # outputs a (N, 10) Tensor
        output = self.fc3(f6) # 分类
        return output
		# 一直在把前面的输出作为下一步的输入

net = Net()
print(net)
```
##### How to know `self.fc1 = nn.Linear(16 * 5 * 5, 120)`?
- 卷积

$$H_{out} = \lfloor \frac{H_{in} - \text{kernel\\_size} + 2 \times \text{padding}}{\text{stride}} \rfloor + 1$$
- 池化

$$H_{out} = \lfloor \frac{H_{in} - \text{kernel\\_size}}{\text{stride}} \rfloor + 1$$
- 全连接

$$\text{Linear\\_Input} = \text{Channels} \times \text{Height} \times \text{Width}$$
##### The relation between the Network and CNN
- Network call CNN to convert input to output in forward and backwards
- CNN: **convolution**->**pooling**->**fully connected**->**flatten**
- Network:**grad_zero->forward（CNN）->compute loss->backward（CNN）->update(optimize)**
#####  Details
- You **just have to define the `forward` function**, and the `backward` function (where gradients are computed) is automatic.
- The learnable parameters of a model are returned by `net.parameters()`
```python
params = list(net.parameters())
print(len(params))
print(params[0].size())  # conv1's .weight
```
- `torch.nn` only supports mini-batches
- `nn.Module` - Neural network module. _Convenient way of encapsulating parameters_, with helpers for **moving them to GPU**, exporting, loading, etc.
#### Loss Function
##### **conception** 
- computes a value that estimates how far away the output is from the target.
```python
output = net(input)
target = torch.randn(10)  # a dummy target, for example
target = target.view(1, -1)  # make it the same shape as output
criterion = nn.MSELoss() # a simple loss function

loss = criterion(output, target)
print(loss)
```
##### **procedure**
- follow `loss` in the backward direction, using its `.grad_fn` attribute, you will see a graph of computations that looks like this:
```
input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d
      -> flatten -> linear -> relu -> linear -> relu -> linear
      -> MSELoss
      -> loss
```
- Then all Tensors in the graph that have `requires_grad=True` will have their `.grad` Tensor accumulated with the gradient.
#### Backprop
- Backword,略
#### Update the weights
The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):
$$weight = weight - learning_rate * gradient$$
##### SGD Implementation:
```python
learning_rate = 0.01
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)
```
##### Other Implementation
```python
import torch.optim as optim

# create your optimizer
optimizer = optim.SGD(net.parameters(), lr=0.01) # 改这里就行

# in your training loop:
optimizer.zero_grad()   # zero the gradient buffers
output = net(input)
loss = criterion(output, target)
loss.backward()
optimizer.step()    # Does the update
```
### Training a Classifier
#### Data-Load and normalize CIFAR10
python packages that convert data into tensor:
- For images, packages such as Pillow, OpenCV are useful
- For audio, packages such as scipy and librosa
- For text, either raw Python or Cython based loading, or NLTK and SpaCy are useful
##### torchvision-a package for common datasets
- `torchvision.datasets` and `torch.utils.data.DataLoader`.
#### Define a Convolutional Neural Network
#### Define a Loss function and optimizer
#### Train the network
#### Test the network on the test data
#### Training on GPU
